# nltk
from nltk import word_tokenize

# numpy
import numpy as np

# Python stdlib
import math
import re

# our imports
from dsrt.config.defaults import ConversationConfig

class Conversation:
    def __init__(self, encoder, decoder, vectorizer, config=ConversationConfig()):
        self.config = config
        
        self.encoder = encoder
        self.decoder = decoder
        self.vectorizer = vectorizer
        self.max_utterance_length = self.vectorizer.properties['max-utterance-length']
        
        self.history = []
        self.context_history = []
        
        
    def start(self, user_utterance=None):
        self.converse(user_utterance)
    
    def converse(self, user_utterance=None):
        if user_utterance is None:
            user_utterance = input("> ")
        
        user_utterance = user_utterance.lower()
        
        self.history.append(user_utterance)
        
        while not re.search(u'\s*(#exit)', user_utterance, re.I):
            # get network's response
            response = self.get_response(user_utterance)
            
            # add it to the history and display to the user
            self.history.append("Machine > " + response)
            print("Computer:\t'{}'".format(response))
            
            # get the user's next utterance
            user_utterance = input("> ").lower()
            self.history.append("User > " + user_utterance)
            
        print("\nConversation history:")
        for turn in self.history:
            print(turn)
        print("\n")

        # for temporary testing, save the utterance history and the context vectors
        with open('utterance_history', 'w+') as f:
            for turn in self.history:
                f.write(turn + '\n')

        with open('context_history', 'w+') as f:
            for context in self.context_history:
                f.write(context.__repr__() + '\n')
    
    def get_response(self, utterance):
        # first tokenize the utterance
        utterance = word_tokenize(utterance)[:self.max_utterance_length]
        print("User:\t\t'{}'".format(utterance))

        # for testing, keep a copy of the string input ...
        copy = utterance
        
        # then vectorize the utterance
        utterance = self.vectorizer.vectorize_utterance(utterance)
        
        # then invoke the model's predict function
        print("\t\t {}".format(utterance))
        (response, context) = self.predict(utterance, copy)

        self.context_history.append(context)
        
        # then devectorize the model's prediction and return it as a string
        print (response)
        print (type(response))
        return ' '.join(self.vectorizer.devectorize_utterance(response))
    
    def predict(self, x, string_x="[not provided]"):
        """
        Take in an integer-vectorized (i.e., index) vector, and predict the maximally
        likely response, returning it as an integer-vectorized (i.e., index) vector.
        """
        recurrent_unit = self.config['recurrent-unit-type']
        
        # encode the input seq into a context vector
        if recurrent_unit == 'lstm':
            context_state = self.encoder.predict(np.array(x))
        elif recurrent_unit == 'gru':
            hidden_state = self.encoder.predict(np.array(x))
            context_state = [hidden_state]
        else:
            raise Exception('Invalid recurrent unit type: {}'.format(recurrent_unit))

        print("Context state from encoding user input <{}> was:\n\t{}\n".format(string_x, context_state))

        # for testing purposes, keep a copy of the context generated by the user input ...
        copy = context_state
        
        # create an empty target sequence, seeded with the start character
        y = self.vectorizer.vectorize_utterance([self.config['start']])
        response = []
        
        i = 0
        while True: 
            # decode the current sequence + current context into a
            # conditional distribution over next token:
            output_token_probs = None
            if recurrent_unit == 'lstm':
                output_token_probs, h, c = self.decoder.predict([y] + context_state)
                context_state = [h, c]
            elif recurrent_unit == 'gru':
                output_token_probs, hidden_state = self.decoder.predict([y] + context_state)
                context_state = [hidden_state]
            else:
                raise Exception('Invalid recurrent unit type: {}'.format(recurrent_unit))
            
            # sample a token from the output distribution (currently using maximum-likelihoo -- i.e., argmax)
            sampled_token = np.argmax(output_token_probs[0, -1, :])
            
            # add the sampled token to our output string
            response += [sampled_token]
            print("i={}, t={}".format(i, sampled_token))
            i = i + 1
            
            # exit condition: either we've
            # - hit the max length (self.data.output_max_len), or
            # - decoded a stop token ('\n')
            if (sampled_token == self.vectorizer.ie.transform([self.config['stop']]) or 
                len(response) >= self.config['max-utterance-length']):
                break
                
            # update the np array (target seq)
            y = np.array([sampled_token])
            
        return (response[:-1], copy)
