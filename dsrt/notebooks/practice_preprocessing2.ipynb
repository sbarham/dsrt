{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "import logging\n",
    "import re\n",
    "import copy\n",
    "\n",
    "# our own imports\n",
    "from config import Config\n",
    "\n",
    "\n",
    "class DialogueSampleSet:\n",
    "    \"\"\"\n",
    "    Fields:\n",
    "        #dialogues             the list of tokenized dialogues\n",
    "        #length                the length of the sampleset (i.e., the number of dialogues)\n",
    "        #max_dialogue_length   the length of the longest dialogue in all samplesets\n",
    "        #max_utterance_length  the length of the longest utterance in all samplesets\n",
    "        #encoder_x             the inputs to the encoder\n",
    "        #decoder_x             the inputs to the decoder\n",
    "        #decoder_y             the target output of the decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, dialogues=[], config=Config()):\n",
    "        self.dialogues = dialogues\n",
    "        self.length = len(dialogues)\n",
    "        self.config = config\n",
    "        self.init_logger()\n",
    "        \n",
    "        # reserved vocabulary items\n",
    "        self.pad_u = '<pad_u>'\n",
    "        self.pad_d = '<pad_d>'\n",
    "        self.start = '<start>'\n",
    "        self.stop = '<stop>'\n",
    "        self.unk = '<unk>'\n",
    "        \n",
    "        # these are set in #dialogues_to_adjacency_pairs()\n",
    "        self.encoder_x = []\n",
    "        self.decoder_x = []\n",
    "        self.decoder_y = []\n",
    "        \n",
    "        self.record_sequence_lengths()\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def init_logger(self):\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(self.config['logging-level'])\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def pad_pair_and_prepare(self):\n",
    "        # pad the dialogues, pair them up (for non-hierarchical models), and\n",
    "        # prepare the encoder/decoder inputs/targets\n",
    "        self.pad_dialogues()\n",
    "        self.dialogues_to_adjacency_pairs()\n",
    "        self.make_encoder_decoder_split()\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def record_sequence_lengths(self):\n",
    "        # record original sequence lengths at the utterance and dialogue level\n",
    "        self.dialogue_lengths = []\n",
    "#         self.utterance_lengths = []\n",
    "        self.utterance_lengths_flat = []\n",
    "        \n",
    "        for dialogue in self.dialogues:\n",
    "            # get dialogue length\n",
    "            self.dialogue_lengths += [len(dialogue)]\n",
    "\n",
    "            # get constituent utterances lengths\n",
    "            lens = [len(u) for u in dialogue]\n",
    "#             self.utterance_lengths += [lens]\n",
    "            self.utterance_lengths_flat += lens\n",
    "        \n",
    "        self.max_dialogue_length = max(self.dialogue_lengths)\n",
    "        self.max_utterance_length = max(self.utterance_lengths_flat)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def pad_dialogues(self):\n",
    "        \"\"\"\n",
    "        Pad the entire dataset.\n",
    "        This involves adding padding at the end of each sentence, and in the case of\n",
    "        a hierarchical model, it also involves adding padding at the end of each dialogue,\n",
    "        so that every training sample (dialogue) has the same dimension.\n",
    "        The padded dialogues are then prepared for their role as encoder_x, decoder_x, and\n",
    "        decoder_y (encoder and decoder inputs, and decoder targets).\n",
    "        \"\"\"\n",
    "        self.log('info', 'Padding the dialogues ...')\n",
    "        \n",
    "        if self.config['hierarchical']:\n",
    "            empty_turn = [self.pad_d] * (self.max_utterance_length + 1)\n",
    "        \n",
    "        for i, d in enumerate(self.dialogues):\n",
    "            for j, u in enumerate(d):\n",
    "                dif = self.max_utterance_length - len(u) + 1\n",
    "                self.dialogues[i][j] += [self.pad_u] * dif\n",
    "#                 self.utterance_lengths[i][j] = len(self.dialogues[i][j])\n",
    "        \n",
    "#         std = self.max_utterance_length + 1\n",
    "#         for i, lens in enumerate(self.utterance_lengths):\n",
    "#             for j, length in enumerate(lens):\n",
    "#                 dif = self.max_utterance_length - length + 1\n",
    "#                 self.dialogues[i][j] += [self.pad_u] * dif\n",
    "#                 self.utterance_lengths[i][j] = len(self.dialogues[i][j])\n",
    "                \n",
    "            # only pad the dialogue if we're training a hierarchical model\n",
    "            if self.config['hierarchical']:\n",
    "                dif = self.max_dialogue_length - self.dialogue_lengths[i]\n",
    "                self.dialogues[i] += [empty_turn] * dif\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def dialogues_to_adjacency_pairs(self):\n",
    "        if self.config['hierarchical']:\n",
    "            pass\n",
    "        self.dialogues = [ap for d in self.dialogues for ap in self.dialogue_to_adjacency_pairs(d)]\n",
    "        \n",
    "        return\n",
    "\n",
    "    def dialogue_to_adjacency_pairs(self, dialogue):\n",
    "        adjacency_pairs = []\n",
    "        for i in range(len(dialogue)):\n",
    "            if i + 1 < len(dialogue):\n",
    "                adjacency_pairs += [[dialogue[i], dialogue[i + 1]]]\n",
    "        \n",
    "        return adjacency_pairs\n",
    "    \n",
    "    def make_encoder_decoder_split(self):\n",
    "        \"\"\"\n",
    "        For now, this assumes a flat (non-hierarchical) model, and therefore\n",
    "        assumes that dialogues are simply adjacency pairs.\n",
    "        \"\"\"\n",
    "        # prepare the encoder_x\n",
    "        self.encoder_x = copy.deepcopy([pair[0] for pair in self.dialogues])\n",
    "        \n",
    "        decoder = [pair[1] for pair in self.dialogues]\n",
    "        \n",
    "        # prepare decoder_x (prefix the <start> symbol to every second-pair part)\n",
    "        self.decoder_x = copy.deepcopy([list([self.start] + u)[:-1] for u in decoder])\n",
    "        \n",
    "        # prepare decoder_y (postfix the <stop> symbol to every second-pair part)\n",
    "        self.decoder_y = copy.deepcopy(decoder)\n",
    "        for i in range(len(self.decoder_y)):\n",
    "            self.decoder_y[i][-1] = self.stop\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def log(self, priority, msg):\n",
    "        \"\"\"\n",
    "        Just a wrapper, for convenience.\n",
    "        NB1: priority may be set to one of:\n",
    "        - CRITICAL     [50]\n",
    "        - ERROR        [40]\n",
    "        - WARNING      [30]\n",
    "        - INFO         [20]\n",
    "        - DEBUG        [10]\n",
    "        - NOTSET       [0]\n",
    "        Anything else defaults to [20]\n",
    "        NB2: the levelmap is a defaultdict stored in Config; it maps priority\n",
    "             strings onto integers\n",
    "        \"\"\"\n",
    "        # self.logger.log(self.config.levelmap[priority], msg)\n",
    "        self.logger.log(logging.CRITICAL, msg)\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class DialogueCorpus:\n",
    "    def __init__(self, config=Config()):\n",
    "        # load configuration\n",
    "        self.config = config\n",
    "        \n",
    "        # reserved vocabulary items\n",
    "        self.pad_u = '<pad_u>'\n",
    "        self.pad_d = '<pad_d>'\n",
    "        self.start = '<start>'\n",
    "        self.stop = '<stop>'\n",
    "        self.unk = '<unk>'\n",
    "        \n",
    "        # init logger\n",
    "        self.init_logger()\n",
    "        \n",
    "        self.log('info', 'Logger initialized')\n",
    "        self.log('info', 'Configuration loaded')\n",
    "        self.log('warn', 'Preparing to process the dialogue corpus ...')\n",
    "        \n",
    "        self.corpus_loaded = False\n",
    "        \n",
    "        # initialize training bookkeeping parameters\n",
    "#         self._epochs_completed = 0\n",
    "#         self._index_in_epoch = 0\n",
    "        \n",
    "        # load and tokenize the dataset\n",
    "        self.load_corpus() # <-- tokenization happens here (there's a good reason)\n",
    "        self.load_vocab()\n",
    "        \n",
    "        # tokenize and split\n",
    "        self.split_corpus()\n",
    "        self.compute_max_sequence_lengths()\n",
    "        \n",
    "        # vectorize\n",
    "        self.initialize_encoders()\n",
    "        self.vectorize_corpus()\n",
    "        \n",
    "        # report success!\n",
    "        self.log('warn', 'Corpus succesfully loaded! Ready for training.')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def init_logger(self):\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(self.config['logging-level'])\n",
    "        \n",
    "        return\n",
    "    \n",
    "    ######################\n",
    "    #       LOADING      #\n",
    "    ######################\n",
    "    \n",
    "    def load_corpus(self):\n",
    "        self.log('info', 'Loading the dataset ...')\n",
    "        \n",
    "        corpus = self.config['path-to-corpus']\n",
    "        dialogues = []\n",
    "        \n",
    "        if not self.corpus_loaded:\n",
    "            with open(corpus, 'r') as f:\n",
    "                dialogues = list(f)\n",
    "                \n",
    "        # filter out long dialogues, or dialogues with long utterances\n",
    "        dialogues = self.tokenize_dialogues(dialogues) # <-- we need to tokenize first so the filtering \n",
    "                                                       #     registers the correct utterance lengths\n",
    "        dialogues = self.filter_dialogues_by_length(dialogues)\n",
    "                \n",
    "        # if desired, retain only a subset of the dialogues:\n",
    "        if self.config['restrict-sample-size']:\n",
    "            dialogues = np.random.choice(dialogues, self.config['sample-size'])\n",
    "        \n",
    "        self.dialogues = dialogues\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def reload_corpus(self):\n",
    "        \"\"\"\n",
    "        Reload the dialogue corpus, in case some changes have been made to it since we last\n",
    "        loaded.\n",
    "        \"\"\"\n",
    "        self.corpus_loaded = False\n",
    "        self.load_corpus()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def load_vocab(self):\n",
    "        self.log('info', 'Initializing vocabulary ...')\n",
    "        \n",
    "        reserved_words = set([self.pad_u, self.pad_d, self.start, self.stop, self.unk])\n",
    "        corpus_words = set([w for d in self.dialogues for u in d for w in u])\n",
    "        \n",
    "        self.vocab_set = set.union(reserved_words, corpus_words)\n",
    "        self.vocab_list = list(self.vocab_set)\n",
    "        \n",
    "        self.vocab_size = len(self.vocab_list)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    ######################\n",
    "    #    TOKENIZATION    #\n",
    "    ######################\n",
    "    \n",
    "    def tokenize_corpus(self):\n",
    "        self.log('info', 'Tokenizing the dataset ...')\n",
    "        self.dialogues = self.tokenize_dialogues(self.dialogues)\n",
    "        \n",
    "    def tokenize_dialogues(self, dialogues):\n",
    "        return [self.tokenize_dialogue(d) for d in dialogues]\n",
    "\n",
    "    def tokenize_dialogue(self, dialogue):\n",
    "        utterances = dialogue.split('\\t')[:-1]\n",
    "        return [self.tokenize_utterance(u) for u in utterances]\n",
    "    \n",
    "    def tokenize_utterance(self, utterance):\n",
    "        return utterance.split(' ')\n",
    "    \n",
    "    \n",
    "    ###########################\n",
    "    #   FILTERING BY LENGTH   #\n",
    "    ###########################\n",
    "    \n",
    "    def filter_dialogues_by_length(self, dialogues):\n",
    "        self.log('info', 'Filtering out long samples ...')\n",
    "        \n",
    "        # for filtering out long dialogues and utterances, we'll need these settings:\n",
    "        max_dl = self.config['max-dialogue-length']\n",
    "        use_max_dl = not (self.config['use-corpus-max-dialogue-length'])\n",
    "        max_ul = self.config['max-utterance-length']\n",
    "        use_max_ul = not (self.config['use-corpus-max-utterance-length'])\n",
    "        \n",
    "        filtered_dialogues = []\n",
    "        \n",
    "        # if we're putting a limit on dialogue length, \n",
    "        # iterate through the dialogues ...\n",
    "        if use_max_dl:\n",
    "            for dialogue in dialogues:\n",
    "                # skip it if we're filtering out long dialogues and this one is too long\n",
    "                if len(dialogue) > max_dl:\n",
    "                    continue\n",
    "\n",
    "                # if we're putting a limit on utterance length, \n",
    "                # iterate through utterances in this dialogue ...\n",
    "                keep_dia = True\n",
    "                if use_max_ul:\n",
    "                    for utterance in dialogue:\n",
    "                        # if an utterance is too long, mark this dialogue for exclusion\n",
    "                        if len(utterance) > max_ul:\n",
    "                            keep_dia = False\n",
    "                            break\n",
    "                            \n",
    "                if keep_dia:\n",
    "                    filtered_dialogues += [dialogue]\n",
    "        \n",
    "        return filtered_dialogues\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    #    TRAIN/TEST SPLIT    #\n",
    "    ##########################\n",
    "    \n",
    "    def split_corpus(self):\n",
    "        self.log('info', 'Splitting the corpus into train/test subsets ...')\n",
    "        \n",
    "        # grab some hyperparameters from our config\n",
    "        split = self.config['train-test-split']\n",
    "        rand_state = self.config['random-state']\n",
    "        \n",
    "        # split the corpus into train and test samples\n",
    "        train, test = train_test_split(self.dialogues, train_size=split, random_state=rand_state)\n",
    "        \n",
    "        self.train = DialogueSampleSet(train)\n",
    "        self.test = DialogueSampleSet(test)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    ###################\n",
    "    #     PADDING     #\n",
    "    ###################\n",
    "    \n",
    "    def compute_max_sequence_lengths(self):\n",
    "        self.log('info', 'Recording sequence lengths ...')\n",
    "        \n",
    "        self.max_dialogue_length = max(self.train.max_dialogue_length, self.test.max_dialogue_length)\n",
    "        self.max_utterance_length = max(self.train.max_utterance_length, self.test.max_utterance_length)\n",
    "        \n",
    "        self.train.max_dialogue_length = self.max_dialogue_length\n",
    "        self.test.max_dialogue_length = self.max_dialogue_length\n",
    "        self.train.max_utterance_length = self.max_utterance_length\n",
    "        self.test.max_utterance_length = self.max_utterance_length\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    #################################\n",
    "    #     INTEGER VECTORIZATION     #\n",
    "    #################################\n",
    "    \n",
    "    \"\"\"\n",
    "    A NOTE:\n",
    "    This should have been obvious to the thinking man, but any reasonable dialogue corpus will be\n",
    "    *far* too big to one-hot encode all in one go -- think 10,000 word vocabulary x 4,000,000 words x\n",
    "    4 bytes per ohe-vector entry: that's 10 * 4 * 4 = 160 GB of one-hot vectors. That *will* fit in\n",
    "    our Azure supercomputer's memory (it has a memory of 240GB), but it makes testing impossible\n",
    "    on any other machine (and the Azure machine is far too expensive to use for testing). Instead,\n",
    "    we'll have to vectorize on demand, on the fly -- unless we encode sentences as integer (index)\n",
    "    sequences, and feed these into a Keras Embedding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize_encoders(self):\n",
    "        \"\"\"\n",
    "        Initialize the integer encoder and the one-hot encoder, fitting them to the vocabulary\n",
    "        of the corpus.\n",
    "        \n",
    "        NB:\n",
    "        From here on out,\n",
    "            - 'ie' stands for 'integer encoded', and\n",
    "            - 'ohe' stands for 'one-hot encoded'\n",
    "        \"\"\"\n",
    "        self.log('info', 'Initializing the encoders ...')\n",
    "        \n",
    "        # create the integer encoder and fit it to our corpus' vocab\n",
    "        self.ie = LabelEncoder()\n",
    "        self.ie_vocab = self.ie.fit_transform(self.vocab_list)\n",
    "        \n",
    "        self.pad_u_index = self.ie.transform([self.pad_u])[0]\n",
    "        \n",
    "        # create the OHE encoder and fit it to our corpus' vocab\n",
    "        self.ohe = OneHotEncoder(sparse=False)\n",
    "        self.ohe_vocab = self.ohe.fit_transform(self.ie_vocab.reshape(len(self.ie_vocab), 1))\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def vectorize_corpus(self):\n",
    "        \"\"\"\n",
    "        Vectorize the entire dataset using integer (index) encoding.\n",
    "        \"\"\"\n",
    "        self.log('info', 'Vectorizing the dialogues (this may take a while) ...')\n",
    "        \n",
    "        # pad the dialogues, pair them up (for non-hierarchical models), and\n",
    "        # prepare the encoder/decoder inputs/targets\n",
    "        self.train.pad_pair_and_prepare()\n",
    "        self.test.pad_pair_and_prepare()\n",
    "        \n",
    "        self.remember_ex = self.train.encoder_x\n",
    "        self.remember_dx = self.train.decoder_x\n",
    "        self.remember_dy = self.train.decoder_y\n",
    "        \n",
    "        # vectorize train samples\n",
    "        self.train.encoder_x = np.array([self.vectorize_utterance(u) for u in self.train.encoder_x])\n",
    "        self.train.decoder_x = np.array([self.vectorize_utterance(u) for u in self.train.decoder_x])\n",
    "        self.train.decoder_y = np.array([self.vectorize_utterance(u) for u in self.train.decoder_y])\n",
    "        self.train.decoder_y_ohe = np.array([self.ie_to_ohe_utterance(u) for u in self.train.decoder_y])\n",
    "        \n",
    "        # vectorize test samples\n",
    "        self.test.encoder_x = np.array([self.vectorize_utterance(u) for u in self.test.encoder_x])\n",
    "        self.test.decoder_x = np.array([self.vectorize_utterance(u) for u in self.test.decoder_x])\n",
    "        self.test.decoder_y = np.array([self.vectorize_utterance(u) for u in self.test.decoder_y])\n",
    "        self.test.decoder_y_ohe = np.array([self.ie_to_ohe_utterance(u) for u in self.test.decoder_y])\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def vectorize_dialogues(self, dialogues):\n",
    "        \"\"\"\n",
    "        Take in a list of dialogues and vectorize them all\n",
    "        \"\"\"\n",
    "        return [self.vectorize_dialogue(d) for d in dialogues]\n",
    "    \n",
    "    def vectorize_dialogue(self, dialogue):\n",
    "        \"\"\"\n",
    "        Take in a dialogue (a sequence of tokenized utterances) and transform it into a \n",
    "        sequence of sequences of indices\n",
    "        \"\"\"\n",
    "        return [self.vectorize_utterance(u) for u in dialogue]\n",
    "    \n",
    "    def vectorize_utterance(self, utterance):\n",
    "        \"\"\"\n",
    "        Take in a tokenized utterance and transform it into a sequence of indices\n",
    "        \"\"\"\n",
    "        for i, word in enumerate(utterance):\n",
    "            if not word in self.vocab_list:\n",
    "                utterance[i] = '<unk>'\n",
    "        \n",
    "        return self.swap_pad_and_zero(self.ie.transform(utterance))\n",
    "    \n",
    "    def devectorize_dialogue(self, dialogue):\n",
    "        \"\"\"\n",
    "        Take in a dialogue of ohe utterances and transform them into a tokenized dialogue\n",
    "        \"\"\"\n",
    "        return [self.devectorize_utterance(u) for u in dialogue]\n",
    "    \n",
    "    def devectorize_utterance(self, utterance):\n",
    "        \"\"\"\n",
    "        Take in a sequence of indices and transform it back into a tokenized utterance\n",
    "        \"\"\"\n",
    "        utterance = self.swap_pad_and_zero(utterance)\n",
    "        return self.ie.inverse_transform(utterance)\n",
    "    \n",
    "    #################################\n",
    "    #       OHE VECTORIZATION       #\n",
    "    #################################\n",
    "    \n",
    "    \n",
    "    def ie_to_ohe_utterance(self, utterance):\n",
    "        return self.ohe.transform(utterance.reshape(len(utterance), 1))\n",
    "    \n",
    "    def vectorize_batch_ohe(self, batch):\n",
    "        \"\"\"\n",
    "        One-hot vectorize a whole batch of dialogues\n",
    "        \"\"\"\n",
    "        return np.array([self.vectorize_dialogue_ohe(dia) for dia in batch])\n",
    "    \n",
    "    def vectorize_dialogue_ohe(self, dia):\n",
    "        \"\"\"\n",
    "        Take in a dialogue (a sequence of tokenized utterances) and transform it into a \n",
    "        sequence of sequences of one-hot vectors\n",
    "        \"\"\"\n",
    "        # we squeeze it because it's coming out with an extra empty\n",
    "        # dimension at the front of the shape: (1 x dia x utt x word)\n",
    "        return np.array([[self.vectorize_utterance_ohe(utt) for utt in dia]]).squeeze()\n",
    "    \n",
    "    def vectorize_utterance_ohe(self, utterance):\n",
    "        \"\"\"\n",
    "        Take in a tokenized utterance and transform it into a sequence of one-hot vectors\n",
    "        \"\"\"\n",
    "        for i, word in enumerate(utterance):\n",
    "            if not word in self.vocab_list:\n",
    "                utterance[i] = '<unk>'\n",
    "        \n",
    "        ie_utterance = self.swap_pad_and_zero(self.ie.transform(utterance))\n",
    "        ohe_utterance = np.array(self.ohe.transform(ie_utterance.reshape(len(ie_utterance), 1)))\n",
    "        \n",
    "        return ohe_utterance\n",
    "    \n",
    "    def devectorize_dialogue_ohe(self, ohe_dialogue):\n",
    "        \"\"\"\n",
    "        Take in a dialogue of ohe utterances and transform them into a tokenized dialogue\n",
    "        \"\"\"\n",
    "        return [self.devectorize_utterance_ohe(u) for u in ohe_dialogue]\n",
    "    \n",
    "    def devectorize_utterance_ohe(self, ohe_utterance):\n",
    "        \"\"\"\n",
    "        Take in a sequence of one-hot vectors and transform it into a tokenized utterance\n",
    "        \"\"\"\n",
    "        ie_utterance = [argmax(w) for w in ohe_utterance]\n",
    "        utterance = self.ie.inverse_transform(self.swap_pad_and_zero(ie_utterance))\n",
    "        \n",
    "        return utterance\n",
    "    \n",
    "    ###################\n",
    "    #     MASKING     #\n",
    "    ###################\n",
    "    \n",
    "    def swap_pad_and_zero(self, utterance):\n",
    "        for i, w in enumerate(utterance):\n",
    "            if w == 0:\n",
    "                utterance[i] = self.pad_u_index\n",
    "            elif w == self.pad_u_index:\n",
    "                utterance[i] = 0\n",
    "        \n",
    "        return utterance\n",
    "    \n",
    "    \n",
    "    ###################\n",
    "    #    UTILITIES    #\n",
    "    ###################\n",
    "    \n",
    "    def log(self, priority, msg):\n",
    "        \"\"\"\n",
    "        Just a wrapper, for convenience.\n",
    "        NB1: priority may be set to one of:\n",
    "        - CRITICAL     [50]\n",
    "        - ERROR        [40]\n",
    "        - WARNING      [30]\n",
    "        - INFO         [20]\n",
    "        - DEBUG        [10]\n",
    "        - NOTSET       [0]\n",
    "        Anything else defaults to [20]\n",
    "        NB2: the levelmap is a defaultdict stored in Config; it maps priority\n",
    "             strings onto integers\n",
    "        \"\"\"\n",
    "        # self.logger.log(self.config.levelmap[priority], msg)\n",
    "        self.logger.log(logging.CRITICAL, msg)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def pretty_print_dialogue(self, dia):\n",
    "        for utt in dia:\n",
    "            if utt[0] == self.pad_d:\n",
    "                break\n",
    "            print(self.stringify_utterance(utt))\n",
    "                \n",
    "        return\n",
    "                      \n",
    "    def stringify_utterance(self, utt):\n",
    "        return ' '.join([w for w in utt if not w == self.pad_u])\n",
    "    \n",
    "    \n",
    "    #################\n",
    "    #   BATCHING    #\n",
    "    #################\n",
    "    \n",
    "#     def next_batch(self):\n",
    "#         start = self._index_in_epoch\n",
    "        \n",
    "#         # Shuffle for the first epoch\n",
    "#         if self._epochs_completed == 0 and start == 0 and self.config['shuffle']:\n",
    "#             perm = np.arange(self.num_train_samples)\n",
    "#             np.random.shuffle(perm)\n",
    "#             self._train = self.t_train_dia[perm]\n",
    "#             self._seqlens = [self.train_utt_seqlens[i] for i in perm]\n",
    "        \n",
    "#         # If we're out of training samples ...\n",
    "#         if start + self.config['batch-size'] > self.num_train_samples:\n",
    "#             # ... then we've finished the epoch\n",
    "#             self._epochs_completed += 1\n",
    "            \n",
    "#             # Gather the leftover dialogues from this epoch\n",
    "#             num_leftover_samples = self.num_train_samples - start\n",
    "#             leftover_dialogues = self._train[start:self.num_train_samples]\n",
    "#             leftover_seqlens = self._seqlens[start:self.num_train_samples]\n",
    "            \n",
    "#             # Get a new permutation of the training dialogues\n",
    "#             if self.config['shuffle']:\n",
    "#                 perm = numpy.arange(self.num_train_samples)\n",
    "#                 np.random.shuffle(perm)\n",
    "#                 self._train = self.t_train_dia[perm]\n",
    "#                 self._seqlens = [self.train_utt_seqlens[i] for i in perm]\n",
    "                \n",
    "#             # Start next epoch\n",
    "#             start = 0\n",
    "#             self._index_in_epoch = batch_size - rest_num_examples\n",
    "#             end = self._index_in_epoch\n",
    "            \n",
    "#             # Put together our batch from leftover and new dialogues\n",
    "#             new_dialogues = self._train[start:end]\n",
    "#             new_seqlens = self._seqlens[start:end]\n",
    "#             batch = np.concatenate((leftover_dialogues, new_dialogues), axis=0)\n",
    "#             seqlens = np.concatenate((leftover_seqlens, new_seqlens), axis=0)\n",
    "            \n",
    "#             # prepare the decoder input/output\n",
    "#             #TODO\n",
    "            \n",
    "#             # release the processed batch\n",
    "#             return (self.vectorize_batch_ohe(batch), seqlens)\n",
    "#         else:\n",
    "#             # update the current index in the training data\n",
    "#             end = self._index_in_epoch + self.config['batch-size']\n",
    "#             self._index_in_epoch = end\n",
    "            \n",
    "#             # get the next batch\n",
    "#             batch = self._train[start:end]\n",
    "#             seqlens = self._seqlens[start:end]\n",
    "            \n",
    "#             # prepare the decoder input/output\n",
    "#             #TODO\n",
    "            \n",
    "#             # release the processed batch\n",
    "#             return (self.vectorize_batch_ohe(batch), seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logger initialized\n",
      "Configuration loaded\n",
      "Preparing to process the dialogue corpus ...\n",
      "Loading the dataset ...\n",
      "Filtering out long samples ...\n",
      "Initializing vocabulary ...\n",
      "Splitting the corpus into train/test subsets ...\n",
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "Recording sequence lengths ...\n",
      "Initializing the encoders ...\n",
      "Vectorizing the dialogues (this may take a while) ...\n",
      "Padding the dialogues ...\n",
      "Padding the dialogues ...\n",
      "Corpus succesfully loaded! Ready for training.\n"
     ]
    }
   ],
   "source": [
    "data = DialogueCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.test.max_utterance_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.max_utterance_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for u in [u for d in data.train.dialogues for u in d]:\n",
    "#     print(len(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16, 110,  74, ...,   0,   0,   0],\n",
       "       [ 16, 229,  18, ...,   0,   0,   0],\n",
       "       [ 16, 517,   9, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [ 16, 407, 492, ...,   0,   0,   0],\n",
       "       [ 16, 316,   9, ...,   0,   0,   0],\n",
       "       [ 16, 182, 314, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.decoder_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pad_u_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ie.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<start>', 'curly', 'bill', ',', 'huh', '?', 'who', 'was', 'that',\n",
       "       'other', 'idiot', '?', '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>',\n",
       "       '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>',\n",
       "       '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>',\n",
       "       '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>',\n",
       "       '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>', '<pad_u>'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.devectorize_utterance(data.train.decoder_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
